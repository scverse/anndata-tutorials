{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1446b800-7ded-4a4c-b54c-8510100c3bda",
   "metadata": {},
   "source": [
    "# On-Disk Concatenation of AnnData Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4bd620",
   "metadata": {},
   "source": [
    "**Author:** Selman Ã–zleyen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b747f-4384-4b16-8f4f-806edfdc0b06",
   "metadata": {},
   "source": [
    "\n",
    "## Initializing\n",
    "\n",
    "Let's begin by importing the necessary libraries and modules. This notebook also uses the [memory-profiler](https://pypi.org/project/memory-profiler/) extension. Ensure you've installed it using `pip install memory-profiler` before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7197c",
   "metadata": {},
   "source": [
    "This notebook uses the [memory-profiler](https://pypi.org/project/memory-profiler/) extension, call `pip install memory-profiler` before running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f65fb557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_profiler import memory_usage\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from anndata.tests.helpers import gen_typed_df\n",
    "from anndata.experimental import write_elem\n",
    "import zarr\n",
    "import anndata\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "import anndata\n",
    "import zarr\n",
    "import gc\n",
    "from anndata.experimental import concat_on_disk\n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f991e3b",
   "metadata": {},
   "source": [
    "## Data Creation and Analysis\n",
    "\n",
    "In this section, we'll demonstrate the core functionality of the `concat_on_disk` method. We'll create datasets and analyze how this method performs in terms of memory usage. This will help us understand its efficiency and benefits, especially when working with large datasets.\n",
    "\n",
    "We will define parameters that will influence the structure of our datasets:\n",
    "\n",
    "- **Shapes**: Defines the shape of array (e.g., \"fat\", \"tall\", \"square\").\n",
    "- **Sizes**: The size of the array, indicating the number of elements.\n",
    "- **Densities**: Specifies the data density. 1 means dense numpy array.\n",
    "\n",
    "These parameters will be utilized in subsequent sections to generate and analyze datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f0bd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directory where the data will be stored\n",
    "OUTDIR = Path(\"tmpdata\")\n",
    "\n",
    "# Parameters that will influence the structure and size of our datasets:\n",
    "\n",
    "# Shapes of the arrays: \"fat\", \"tall\", or \"square\"\n",
    "shapes = [\"fat\", \"tall\", \"square\"]\n",
    "\n",
    "# Sizes of the dataset, indicating the number of elements\n",
    "sizes = [10_000]\n",
    "\n",
    "# Densities: Specifies the data density. A higher value means more non-zero elements\n",
    "densities = [0.1, 1]\n",
    "\n",
    "# Number of times each array type will be created\n",
    "num_runs = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dbc5a4",
   "metadata": {},
   "source": [
    "### create_adata\n",
    "\n",
    "This function is designed to create an `AnnData` object, which is a foundational data structure used in bioinformatics to store high-dimensional data such as gene expression matrices. Given a data matrix `X` and its shape, the function constructs the `AnnData` object complete with observation (`obs`) and variable (`var`) metadata.\n",
    "\n",
    "- `shape`: The shape (dimensions) of the data matrix.\n",
    "- `X`: The actual data matrix (could be dense or sparse).\n",
    "\n",
    "Returns: An `AnnData` object constructed from the input data and metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e919ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adata(shape, X):\n",
    "    # Shape of the data matrix\n",
    "    M, N = shape\n",
    "    \n",
    "    # Generating observation and variable names\n",
    "    obs_names = pd.Index(f\"cell{i}\" for i in range(shape[0]))\n",
    "    var_names = pd.Index(f\"gene{i}\" for i in range(shape[1]))\n",
    "    \n",
    "    # Creating observation and variable dataframes\n",
    "    obs = gen_typed_df(M, obs_names)\n",
    "    var = gen_typed_df(N, var_names)\n",
    "    \n",
    "    # Renaming columns to ensure uniqueness\n",
    "    obs.rename(columns=dict(cat=\"obs_cat\"), inplace=True)\n",
    "    var.rename(columns=dict(cat=\"var_cat\"), inplace=True)\n",
    "    \n",
    "    # Constructing the AnnData object\n",
    "    adata = anndata.AnnData(X, obs=obs, var=var)\n",
    "    adata.var_names_make_unique()\n",
    "    adata.obs_names_make_unique()\n",
    "\n",
    "    return adata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9449622e",
   "metadata": {},
   "source": [
    "### generate_array_funcs_and_names\n",
    "\n",
    "This function determines the type of array functions and their corresponding names based on the provided `density` parameter. It essentially helps in figuring out if the dataset is dense or sparse.\n",
    "\n",
    "- `density`: The density of the dataset. If the density is 1, the dataset is dense; otherwise, it's sparse.\n",
    "\n",
    "Returns: A tuple containing the list of array functions and their corresponding names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2eb98ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_array_funcs_and_names(density):\n",
    "    array_funcs = []  # List to hold array functions\n",
    "    array_names = []  # List to hold array names\n",
    "    \n",
    "    # Check if dataset is dense\n",
    "    if density == 1:\n",
    "        array_names.append(\"np\")\n",
    "        array_funcs.append(lambda x: x.toarray())\n",
    "    else:\n",
    "        # For sparse datasets, consider both csc and csr formats\n",
    "        array_names.extend([\"csc\", \"csr\"])\n",
    "        array_funcs.extend([sparse.csc_matrix, sparse.csr_matrix])\n",
    "    \n",
    "    return array_funcs, array_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9786c59",
   "metadata": {},
   "source": [
    "### generate_dimensions\n",
    "\n",
    "Given a shape description (like \"fat\", \"tall\", or \"square\") and a base size, this function computes the exact dimensions \\(M\\) and \\(N\\) of the dataset. \n",
    "\n",
    "- `shape`: Description of the desired shape of the dataset.\n",
    "- `size`: Base size for the dataset.\n",
    "\n",
    "Returns: The dimensions \\(M\\) and \\(N\\) of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dd5068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dimensions(shape, size):\n",
    "    # Default dimensions\n",
    "    M = size\n",
    "    N = size\n",
    "    \n",
    "    # If the shape isn't square, adjust the dimensions\n",
    "    if shape != \"square\":\n",
    "        other_size = size + int(size * np.random.uniform(0.2, 0.4))\n",
    "        if shape == \"fat\":\n",
    "            M = other_size\n",
    "        elif shape == \"tall\":\n",
    "            N = other_size\n",
    "            \n",
    "    return M, N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a00c66",
   "metadata": {},
   "source": [
    "## Writing The Arrays To Disk\n",
    "\n",
    "We will use the functions defined below to write the anndatas. There is no need to understand them all. However, the functions are also explained below for users who would like to create their own datasets to do the measurements.\n",
    "\n",
    "### Functions Overview\n",
    "\n",
    "#### 1. `write_data_to_zarr`\n",
    "\n",
    "This function is responsible for writing a given dataset `X` to a Zarr format file. Zarr is a format for the storage of chunked, compressed, N-dimensional arrays, which is useful for efficient on-disk storage and retrieval of large datasets.\n",
    "\n",
    "- **Parameters**:\n",
    "    - `X`: The dataset to be written.\n",
    "    - `shape`: Descriptive shape of the dataset.\n",
    "    - `array_name`: Name representing the type of array (e.g., \"np\", \"csc\", \"csr\").\n",
    "    - `outdir`: Directory where the Zarr file should be stored.\n",
    "    - `file_id`: Identifier for the file, used in naming.\n",
    "\n",
    "- **Returns**: A string report detailing the writing operation.\n",
    "\n",
    "#### 2. `write_temp_data`\n",
    "\n",
    "This function is designed to write temporary data based on the specified parameters to the output directory. It iteratively generates data sets based on shapes, sizes, densities, and number of runs, and writes each dataset to a Zarr format file using the `write_data_to_zarr` function.\n",
    "\n",
    "- **Parameters**:\n",
    "    - `shapes`: List of dataset shapes (e.g., \"fat\", \"tall\", \"square\").\n",
    "    - `sizes`: List of dataset sizes.\n",
    "    - `densities`: List of dataset densities.\n",
    "    - `num_runs`: Number of iterations for data generation.\n",
    "    - `outdir`: Directory where the Zarr files should be stored.\n",
    "    - `rewrite`: Boolean flag; if True, any existing data in the output directory will be overwritten.\n",
    "\n",
    "This function not only writes the datasets but also maintains a log of the datasets written in a file named \"done.txt\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "134c147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_to_zarr(X, shape, array_name, outdir, file_id):\n",
    "    fname = str(outdir) + f\"/{file_id:02d}_{shape}_{array_name}\"\n",
    "    adata = create_adata((X.shape[0], X.shape[1]), X)\n",
    "    output_zarr_path = f\"{str(fname)}.zarr\"\n",
    "    z = zarr.open_group(output_zarr_path)\n",
    "    write_elem(z, \"/\", adata)\n",
    "    zarr.consolidate_metadata(z.store)\n",
    "    return f\"wrote {X.shape[0]}x{X.shape[1]}_{array_name} -> {fname}\\n\"\n",
    "\n",
    "def write_temp_data(shapes, sizes, densities, num_runs, outdir, rewrite=False):\n",
    "    outdir.mkdir(exist_ok=True)\n",
    "    if rewrite:\n",
    "        (outdir / \"done.txt\").unlink(missing_ok=True)\n",
    "    if (outdir / \"done.txt\").exists():\n",
    "        print(\"already done\")\n",
    "        with open(outdir / \"done.txt\", \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                print(line)\n",
    "        return\n",
    "\n",
    "    saved = []\n",
    "    file_id = 1\n",
    "    for _ in range(num_runs):\n",
    "        for shape in shapes:\n",
    "            for size in sizes:\n",
    "                for density in densities:\n",
    "                    array_funcs, array_names = generate_array_funcs_and_names(density)\n",
    "                    M, N = generate_dimensions(shape, size)\n",
    "\n",
    "                    X_base = sparse.random(M, N, density=density, format=\"csc\")\n",
    "\n",
    "                    for array_func, array_name in zip(array_funcs, array_names):\n",
    "                        X = array_func(X_base)\n",
    "                        report = write_data_to_zarr(X, shape, array_name, outdir, file_id)\n",
    "                        print(report)\n",
    "                        saved.append(report)\n",
    "                        file_id += 1\n",
    "    with open(outdir / \"done.txt\", \"w\") as f:\n",
    "        f.writelines(saved)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf50dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already done\n",
      "wrote 12941x10000_csc -> tmpdata/01_fat_csc\n",
      "\n",
      "wrote 12941x10000_csr -> tmpdata/02_fat_csr\n",
      "\n",
      "wrote 12289x10000_np -> tmpdata/03_fat_np\n",
      "\n",
      "wrote 10000x12366_csc -> tmpdata/04_tall_csc\n",
      "\n",
      "wrote 10000x12366_csr -> tmpdata/05_tall_csr\n",
      "\n",
      "wrote 10000x13624_np -> tmpdata/06_tall_np\n",
      "\n",
      "wrote 10000x10000_csc -> tmpdata/07_square_csc\n",
      "\n",
      "wrote 10000x10000_csr -> tmpdata/08_square_csr\n",
      "\n",
      "wrote 10000x10000_np -> tmpdata/09_square_np\n",
      "\n",
      "wrote 13924x10000_csc -> tmpdata/10_fat_csc\n",
      "\n",
      "wrote 13924x10000_csr -> tmpdata/11_fat_csr\n",
      "\n",
      "wrote 13321x10000_np -> tmpdata/12_fat_np\n",
      "\n",
      "wrote 10000x12377_csc -> tmpdata/13_tall_csc\n",
      "\n",
      "wrote 10000x12377_csr -> tmpdata/14_tall_csr\n",
      "\n",
      "wrote 10000x12595_np -> tmpdata/15_tall_np\n",
      "\n",
      "wrote 10000x10000_csc -> tmpdata/16_square_csc\n",
      "\n",
      "wrote 10000x10000_csr -> tmpdata/17_square_csr\n",
      "\n",
      "wrote 10000x10000_np -> tmpdata/18_square_np\n",
      "\n",
      "wrote 13778x10000_csc -> tmpdata/19_fat_csc\n",
      "\n",
      "wrote 13778x10000_csr -> tmpdata/20_fat_csr\n",
      "\n",
      "wrote 12484x10000_np -> tmpdata/21_fat_np\n",
      "\n",
      "wrote 10000x13293_csc -> tmpdata/22_tall_csc\n",
      "\n",
      "wrote 10000x13293_csr -> tmpdata/23_tall_csr\n",
      "\n",
      "wrote 10000x13809_np -> tmpdata/24_tall_np\n",
      "\n",
      "wrote 10000x10000_csc -> tmpdata/25_square_csc\n",
      "\n",
      "wrote 10000x10000_csr -> tmpdata/26_square_csr\n",
      "\n",
      "wrote 10000x10000_np -> tmpdata/27_square_np\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# You can call the function like this:\n",
    "write_temp_data(shapes, sizes, densities, num_runs, OUTDIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8c7897",
   "metadata": {},
   "source": [
    "### Putting our arrays in categories\n",
    "\n",
    "The `create_datasets` function constructs a dictionary that maps dataset types (dense or sparse) and their axis (0 or 1) to a set of corresponding file paths. The function processes different file sets and, based on conditions like `requires_reindexing`, refines the set of file paths to be associated with each dataset type and axis combination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f71e387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files by properties\n",
    "filesets = {\n",
    "    'nps' : set(glob.glob(str(OUTDIR) + \"/*np*\")),\n",
    "    'csrs' : set(glob.glob(str(OUTDIR) + \"/*csr*\")),\n",
    "    'cscs' : set(glob.glob(str(OUTDIR) + \"/*csc*\")),\n",
    "    'fats' : set(glob.glob(str(OUTDIR) + \"/*fat*\")),\n",
    "    'talls' : set(glob.glob(str(OUTDIR) + \"/*tall*\")),\n",
    "    'squares' :set(glob.glob(str(OUTDIR) + \"/*square*\")),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfbc8bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(filesets, requires_reindexing=False):\n",
    "    data = dict()\n",
    "    for fileset, axis in ((\"cscs\",1), (\"csrs\",0), (\"nps\",0), (\"nps\",1)):\n",
    "        filepaths = filesets[fileset].copy()\n",
    "        if not requires_reindexing:\n",
    "            tall_or_fat = filesets['talls'] if axis == 1 else filesets['fats']\n",
    "            filepaths = filepaths.intersection(tall_or_fat.union(filesets['squares']))\n",
    "        fileset_name = \"dense\" if fileset == \"nps\" else \"sparse\"\n",
    "        data[fileset_name, axis] = filepaths\n",
    "    return data    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee43c851",
   "metadata": {},
   "source": [
    "Below you can see the both the list of anndatas that would require reindexing when concatenating (i.e, their axis size don't match) and the ones who don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7c4769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_aligned, datasets_unaligned =create_datasets(filesets,requires_reindexing=False), create_datasets(filesets, requires_reindexing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e833a1",
   "metadata": {},
   "source": [
    "## Measuring Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f418d1b7",
   "metadata": {},
   "source": [
    "### `get_arr_sizes`\n",
    "\n",
    "This function calculates the size of the data arrays for a list of given file paths. It can accommodate both sparse and dense formats, adjusting the computation method accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "### `get_mem_usage`\n",
    "\n",
    "The function `get_mem_usage` evaluates the memory usage when performing on-disk concatenation using the `concat_on_disk` method. Depending on whether the dataset is sparse or dense, it either initiates a Dask cluster to handle the data or directly concatenates it. It returns the memory increment, the maximum memory used, the memory usage over time, and the initial memory.\n",
    "\n",
    "---\n",
    "\n",
    "### `dataset_max_mem`\n",
    "\n",
    "The `dataset_max_mem` function profiles and prints the maximum memory usage when concatenating datasets of different types (sparse or dense) and along different axes. For each dataset and axis combination, it determines the files to concatenate, calculates their sizes, and then measures the memory usage during the concatenation process. The results are stored in a dictionary that maps the dataset type and axis to the corresponding memory usage metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cf0be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arr_sizes(filepaths, is_sparse):\n",
    "    def get_arr_size(g):\n",
    "        if is_sparse:\n",
    "            size = (\n",
    "                g.store.getsize(\"X/data\")\n",
    "                + g.store.getsize(\"X/indices\")\n",
    "                + g.store.getsize(\"X/indptr\")\n",
    "            )\n",
    "        else:\n",
    "            size = g.store.getsize(\"X\")\n",
    "        return size\n",
    "\n",
    "    return [get_arr_size(zarr.open_group(filepath)) for filepath in filepaths]\n",
    "\n",
    "\n",
    "def get_mem_usage(filepaths, writepth, axis, max_arg, is_sparse):\n",
    "    concat_kwargs = {\n",
    "        \"in_files\": filepaths,\n",
    "        \"out_file\": writepth,\n",
    "        \"axis\": axis,\n",
    "    }\n",
    "\n",
    "    if not is_sparse:\n",
    "        cluster = LocalCluster(n_workers=1, threads_per_worker=1, memory_limit=max_arg)\n",
    "        client = Client(cluster)\n",
    "    else:\n",
    "        concat_kwargs[\"max_loaded_elems\"] = max_arg\n",
    "\n",
    "    # get the current memory usage\n",
    "    initial_mem = memory_usage(-1, interval=0.001)[0]\n",
    "\n",
    "    mem_usages = memory_usage(\n",
    "        (\n",
    "            concat_on_disk,\n",
    "            (),\n",
    "            concat_kwargs,\n",
    "        ),\n",
    "        include_children=True,\n",
    "        interval=0.001,\n",
    "    )\n",
    "    max_mem = max(mem_usages)\n",
    "    mem_increment = max_mem - initial_mem\n",
    "\n",
    "    if not is_sparse:\n",
    "        client.close()\n",
    "        cluster.close()\n",
    "    return mem_increment, max_mem, mem_usages, initial_mem\n",
    "\n",
    "\n",
    "def dataset_max_mem(max_arg, datasets, array_type):\n",
    "    results = {}\n",
    "    is_sparse = array_type == \"sparse\"\n",
    "    for filepaths,axis in [(datasets[array_type,axis],axis) for axis in [0,1]]:\n",
    "        writepth = OUTDIR / f\"{array_type}_{axis}.zarr\"\n",
    "        if writepth.exists():\n",
    "            shutil.rmtree(writepth)\n",
    "\n",
    "        # print the files we are concatenating\n",
    "        print(\"Dataset:\", array_type, axis)\n",
    "        print(f\"Concatenating {len(filepaths)} files with sizes:\")\n",
    "        sizes = get_arr_sizes(filepaths, is_sparse)\n",
    "        print([str(s//(2**20))+'MiB' for s in sizes])\n",
    "        print(f\"Total size: {sum(sizes)//(2**20)}MiB\")\n",
    "        \n",
    "\n",
    "\n",
    "        # force garbage collection\n",
    "        gc.collect()\n",
    "        # perform profiling\n",
    "        mem_increment, max_mem, mem_usages, initial_mem = get_mem_usage(filepaths, writepth, axis, max_arg, is_sparse)\n",
    "        # force garbage collection again\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"Concatenation finished\")\n",
    "        print(\"Max memory increase:\", int(mem_increment), \"MiB\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "        results[array_type, axis] = {\"max_mem\": max_mem, \"increment\": mem_increment}\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73c95fd",
   "metadata": {},
   "source": [
    "## Results of concatenation without reindexing\n",
    "\n",
    "In this section, we evaluate the memory performance of the `concat_on_disk` function when concatenating datasets **without** the need for reindexing. The printed reports provide details about the individual file sizes, the total dataset size, and the maximum memory increment during the concatenation.\n",
    "\n",
    "\n",
    "### Sparse Datasets\n",
    "\n",
    "For sparse datasets:\n",
    "\n",
    "- We can observe that the function has been called multiple times with different memory constraints (`max_arg` values), and each time the datasets were concatenated successfully.\n",
    "- It's crucial to note that even when the combined size of the files exceeds the allocated memory, the concatenation still proceeds efficiently. This behavior highlights the primary advantage of the `concat_on_disk` function: it performs the concatenation **on disk**, ensuring that memory consumption remains low, even for large datasets.\n",
    "  \n",
    "However, it's also worth noting that if one has sufficient memory to fit the files, performing the concatenation in memory would be faster.\n",
    "\n",
    "### Dense Datasets\n",
    "\n",
    "The results for dense datasets follow a similar pattern:\n",
    "\n",
    "- The datasets are concatenated successfully under memory constraints.\n",
    "- The total size of the dataset is much larger than the memory increment, reinforcing the efficiency of on-disk concatenation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad448529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: sparse 0\n",
      "Concatenating 6 files with sizes:\n",
      "['109MiB', '101MiB', '78MiB', '108MiB', '78MiB', '78MiB']\n",
      "Total size: 554MiB\n",
      "Concatenation finished\n",
      "Max memory increase: 160 MiB\n",
      "--------------------------------------------------\n",
      "Dataset: sparse 1\n",
      "Concatenating 6 files with sizes:\n",
      "['78MiB', '78MiB', '78MiB', '97MiB', '97MiB', '104MiB']\n",
      "Total size: 534MiB\n",
      "Concatenation finished\n",
      "Max memory increase: 159 MiB\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('sparse', 0): {'max_mem': 354.98046875, 'increment': 160.40625},\n",
       " ('sparse', 1): {'max_mem': 370.18359375, 'increment': 159.203125}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_max_mem(max_arg=1_000_000_000, datasets=datasets_aligned, array_type='sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3b74ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: dense 0\n",
      "Concatenating 6 files with sizes:\n",
      "['891MiB', '668MiB', '835MiB', '668MiB', '822MiB', '668MiB']\n",
      "Total size: 4556MiB\n",
      "Concatenation finished\n",
      "Max memory increase: 861 MiB\n",
      "--------------------------------------------------\n",
      "Dataset: dense 1\n",
      "Concatenating 6 files with sizes:\n",
      "['910MiB', '668MiB', '843MiB', '668MiB', '668MiB', '923MiB']\n",
      "Total size: 4684MiB\n",
      "Concatenation finished\n",
      "Max memory increase: 1064 MiB\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('dense', 0): {'max_mem': 1084.64453125, 'increment': 861.359375},\n",
       " ('dense', 1): {'max_mem': 1295.5859375, 'increment': 1064.3671875}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_max_mem(max_arg=\"1000MiB\", datasets=datasets_aligned, array_type='dense')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0bbd89",
   "metadata": {},
   "source": [
    "## Results of concatenation with reindexing\n",
    "\n",
    "This section presents the results of the `concat_on_disk` function when concatenating datasets that **require** reindexing.\n",
    "\n",
    "The observations and interpretations for this section are similar to the ones mentioned for the \"without reindexing\" section. The primary difference is the datasets used for the concatenation. Once again, the on-disk concatenation allows for efficient memory usage, even when the datasets need reindexing.\n",
    "\n",
    "One can also see the effect of the memory contrain on the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "781b2ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: sparse 0\n",
      "Concatenating 9 files with sizes:\n",
      "['97MiB', '104MiB', '109MiB', '101MiB', '78MiB', '108MiB', '78MiB', '97MiB', '78MiB']\n",
      "Total size: 853MiB\n",
      "Concatenation finished\n",
      "Max memory increase: 467 MiB\n",
      "--------------------------------------------------\n",
      "Dataset: sparse 1\n",
      "Concatenating 9 files with sizes:\n",
      "['109MiB', '101MiB', '78MiB', '78MiB', '78MiB', '97MiB', '97MiB', '108MiB', '104MiB']\n",
      "Total size: 853MiB\n",
      "Concatenation finished\n",
      "Max memory increase: 631 MiB\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('sparse', 0): {'max_mem': 705.9765625, 'increment': 467.0078125},\n",
       " ('sparse', 1): {'max_mem': 879.07421875, 'increment': 631.796875}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_max_mem(max_arg=1_000_000_000, datasets=datasets_unaligned, array_type='sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28e7d61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: sparse 0\n",
      "Concatenating 9 files with sizes:\n",
      "['97MiB', '104MiB', '109MiB', '101MiB', '78MiB', '108MiB', '78MiB', '97MiB', '78MiB']\n",
      "Total size: 853MiB\n",
      "Concatenation finished\n",
      "Max memory increase: 204 MiB\n",
      "--------------------------------------------------\n",
      "Dataset: sparse 1\n",
      "Concatenating 9 files with sizes:\n",
      "['109MiB', '101MiB', '78MiB', '78MiB', '78MiB', '97MiB', '97MiB', '108MiB', '104MiB']\n",
      "Total size: 853MiB\n",
      "Concatenation finished\n",
      "Max memory increase: 205 MiB\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('sparse', 0): {'max_mem': 455.08984375, 'increment': 204.109375},\n",
       " ('sparse', 1): {'max_mem': 458.26953125, 'increment': 205.38671875}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_max_mem(max_arg=1_000_000, datasets=datasets_unaligned, array_type='sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28426915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: dense 0\n",
      "Concatenating 9 files with sizes:\n",
      "['891MiB', '910MiB', '668MiB', '923MiB', '835MiB', '668MiB', '843MiB', '822MiB', '668MiB']\n",
      "Total size: 7233MiB\n",
      "Concatenation finished\n",
      "Max memory increase: 1056 MiB\n",
      "--------------------------------------------------\n",
      "Dataset: dense 1\n",
      "Concatenating 9 files with sizes:\n",
      "['891MiB', '910MiB', '668MiB', '923MiB', '835MiB', '668MiB', '843MiB', '822MiB', '668MiB']\n",
      "Total size: 7233MiB\n",
      "Concatenation finished\n",
      "Max memory increase: 969 MiB\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('dense', 0): {'max_mem': 1308.2265625, 'increment': 1056.375},\n",
       " ('dense', 1): {'max_mem': 1226.05078125, 'increment': 969.38671875}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_max_mem(max_arg=\"1000MiB\", datasets=datasets_unaligned, array_type='dense')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
