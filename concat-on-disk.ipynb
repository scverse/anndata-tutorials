{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1446b800-7ded-4a4c-b54c-8510100c3bda",
   "metadata": {},
   "source": [
    "# On-Disk Concatenation of AnnData Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4bd620",
   "metadata": {},
   "source": [
    "**Author:** Selman Ã–zleyen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b747f-4384-4b16-8f4f-806edfdc0b06",
   "metadata": {},
   "source": [
    "## Initalizing\n",
    "\n",
    "First let's do our imports and initalize adata objects with the help of the `adata_with_dask` function defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f65fb557",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "from anndata.tests.helpers import gen_typed_df\n",
    "from anndata.experimental import write_elem\n",
    "import zarr\n",
    "import anndata\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import tempfile\n",
    "import dask.distributed as dd\n",
    "\n",
    "import anndata\n",
    "from anndata._core.merge import concat\n",
    "import dask.array as da\n",
    "import zarr\n",
    "\n",
    "from anndata.experimental import read_dispatched, read_elem, concat_on_disk\n",
    "\n",
    "\n",
    "OUTDIR = Path(\"tmpdata\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "shapes = [\"fat\", \"tall\", \"square\"]\n",
    "sizes = [10_000]\n",
    "densities = [0.1, 1]\n",
    "num_runs = 1\n",
    "\n",
    "\n",
    "def create_adata(shape, X):\n",
    "    M, N = shape\n",
    "    obs_names = pd.Index(f\"cell{i}\" for i in range(shape[0]))\n",
    "    var_names = pd.Index(f\"gene{i}\" for i in range(shape[1]))\n",
    "    obs = gen_typed_df(M, obs_names)\n",
    "    var = gen_typed_df(N, var_names)\n",
    "    # For #147\n",
    "    obs.rename(columns=dict(cat=\"obs_cat\"), inplace=True)\n",
    "    var.rename(columns=dict(cat=\"var_cat\"), inplace=True)\n",
    "    return anndata.AnnData(X, obs=obs, var=var)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671b747a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "460ab6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 7475x10000_density=0.1_csc -> tmpdata/01_fat_csc\n",
      "wrote 7346x10000_density=0.1_csr -> tmpdata/02_fat_csr\n",
      "wrote 7401x10000_density=1.0_np -> tmpdata/03_fat_np\n",
      "wrote 10000x7039_density=0.1_csc -> tmpdata/04_tall_csc\n",
      "wrote 10000x7889_density=0.1_csr -> tmpdata/05_tall_csr\n",
      "wrote 10000x8142_density=1.0_np -> tmpdata/06_tall_np\n",
      "wrote 10000x10000_density=0.1_csc -> tmpdata/07_square_csc\n",
      "wrote 10000x10000_density=0.1_csr -> tmpdata/08_square_csr\n",
      "wrote 10000x10000_density=1.0_np -> tmpdata/09_square_np\n"
     ]
    }
   ],
   "source": [
    "file_id = 1\n",
    "for _ in range(num_runs):\n",
    "    for shape in shapes:\n",
    "        for size in sizes:\n",
    "            for density in densities:\n",
    "                is_dense = density == 1\n",
    "                array_funcs = []\n",
    "                array_names = []\n",
    "                if is_dense:\n",
    "                    array_names.append(\"np\")\n",
    "                    array_funcs.append(lambda x: x.toarray())\n",
    "                else:\n",
    "                    array_names.append(\"csc\")\n",
    "                    array_names.append(\"csr\")\n",
    "                    array_funcs.append(sparse.csc_matrix)\n",
    "                    array_funcs.append(sparse.csr_matrix)\n",
    "\n",
    "                for array_func, array_name in zip(array_funcs, array_names):\n",
    "                    M = size\n",
    "                    N = size\n",
    "                    if shape != \"square\":\n",
    "                        other_size = int(size * np.random.uniform(0.7, 0.9))\n",
    "                        if shape == \"fat\":\n",
    "                            M = other_size\n",
    "                        elif shape == \"tall\":\n",
    "                            N = other_size\n",
    "\n",
    "                    X = array_func(\n",
    "                        sparse.random(M, N, density=density, format=\"csc\")\n",
    "                    )\n",
    "                    adata = create_adata(\n",
    "                        (M, N),\n",
    "                        X,\n",
    "                    )\n",
    "                    fname =  str(OUTDIR)+f\"/{file_id:02d}_{shape}_{array_name}\"\n",
    "                    file_id += 1\n",
    "                    print(f\"wrote {M}x{N}_density={density:0.1f}_{array_name} -> {fname}\")\n",
    "                    if is_dense:\n",
    "                        output_zarr_path = f\"{str(fname)}.zarr\"\n",
    "                        z = zarr.open_group(output_zarr_path)\n",
    "\n",
    "                        write_elem(z, \"/\", adata)\n",
    "                        zarr.consolidate_metadata(z.store)\n",
    "                    else:\n",
    "                        adata.write_zarr(f\"{fname}.zarr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f71e387",
   "metadata": {},
   "outputs": [],
   "source": [
    "nps = set(glob.glob(str(OUTDIR) + \"/*np*\"))\n",
    "csrs = set(glob.glob(str(OUTDIR) + \"/*csr*\"))\n",
    "cscs = set(glob.glob(str(OUTDIR) + \"/*csc*\"))\n",
    "fats = set(glob.glob(str(OUTDIR) + \"/*fat*\"))\n",
    "talls = set(glob.glob(str(OUTDIR) + \"/*tall*\"))\n",
    "squares = set(glob.glob(str(OUTDIR) + \"/*square*\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f9f1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tmpdata/03_fat_np.zarr',\n",
       " 'tmpdata/06_tall_np.zarr',\n",
       " 'tmpdata/09_square_np.zarr'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f35076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_on_disk_wrapper(filepaths, writepth, axis):\n",
    "    from multiprocessing import Lock\n",
    "    with dd.LocalCluster(memory_limit=\"400MB\", n_workers=1,threads_per_worker=1) as cluster:\n",
    "        with dd.Client(cluster) as client:\n",
    "            \n",
    "            lock = Lock()\n",
    "            concat_on_disk(filepaths, writepth, axis=axis, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfbc8bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "for axis in (0, 1):\n",
    "    for fileset in (\"csrs\", \"nps-0\", \"nps-1\", \"cscs\"):\n",
    "        filepaths = []\n",
    "        if \"csrs\" in fileset:\n",
    "            filepaths = csrs\n",
    "            axis = 0\n",
    "        elif \"nps\" in fileset:\n",
    "            filepaths = nps\n",
    "            if \"0\" in fileset:\n",
    "                axis = 0\n",
    "            elif \"1\" in fileset:\n",
    "                axis = 1\n",
    "        elif fileset == \"cscs\":\n",
    "            filepaths = cscs\n",
    "            axis = 1\n",
    "\n",
    "        if axis == 0:\n",
    "            filepaths = filepaths.intersection(fats.union(squares))\n",
    "        elif axis == 1:\n",
    "            filepaths = filepaths.intersection(talls.union(squares))\n",
    "\n",
    "        data[fileset] = filepaths, axis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc83fbb8-ab85-40d5-81b9-c85099b1bc97",
   "metadata": {},
   "source": [
    "Here is how our adata looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "769ecca2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "writepth = OUTDIR / \"out.zarr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97ba50d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths, axis = data[\"nps-1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919e19cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8f1528e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 15:37:24,507 - distributed.protocol.pickle - ERROR - Failed to serialize <ToPickle: HighLevelGraph with 1 layers.\n",
      "<dask.highlevelgraph.HighLevelGraph object at 0x7f59aa12c6d0>\n",
      " 0. 140023082158464\n",
      ">.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sel/mambaforge/envs/dask/lib/python3.9/site-packages/distributed/protocol/pickle.py\", line 63, in dumps\n",
      "    result = pickle.dumps(x, **dump_kwargs)\n",
      "TypeError: cannot pickle '_thread.lock' object\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sel/mambaforge/envs/dask/lib/python3.9/site-packages/distributed/protocol/pickle.py\", line 68, in dumps\n",
      "    pickler.dump(x)\n",
      "TypeError: cannot pickle '_thread.lock' object\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sel/mambaforge/envs/dask/lib/python3.9/site-packages/distributed/protocol/pickle.py\", line 81, in dumps\n",
      "    result = cloudpickle.dumps(x, **dump_kwargs)\n",
      "  File \"/home/sel/mambaforge/envs/dask/lib/python3.9/site-packages/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/sel/mambaforge/envs/dask/lib/python3.9/site-packages/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "TypeError: cannot pickle '_thread.lock' object\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "('Could not serialize object of type HighLevelGraph', '<ToPickle: HighLevelGraph with 1 layers.\\n<dask.highlevelgraph.HighLevelGraph object at 0x7f59aa12c6d0>\\n 0. 140023082158464\\n>')\n\nAbove error raised while writing key 'X' of <class 'zarr.hierarchy.Group'> to <zarr.storage.DirectoryStore object at 0x7f59aa13b610>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/dask/lib/python3.9/site-packages/distributed/protocol/pickle.py:63\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(x, buffer_callback, protocol)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     result \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mdumps(x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdump_kwargs)\n\u001b[1;32m     64\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle '_thread.lock' object",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/dask/lib/python3.9/site-packages/distributed/protocol/pickle.py:68\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(x, buffer_callback, protocol)\u001b[0m\n\u001b[1;32m     67\u001b[0m buffers\u001b[39m.\u001b[39mclear()\n\u001b[0;32m---> 68\u001b[0m pickler\u001b[39m.\u001b[39;49mdump(x)\n\u001b[1;32m     69\u001b[0m result \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mgetvalue()\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle '_thread.lock' object",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/dask/lib/python3.9/site-packages/distributed/protocol/serialize.py:350\u001b[0m, in \u001b[0;36mserialize\u001b[0;34m(x, serializers, on_error, context, iterate_collection)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 350\u001b[0m     header, frames \u001b[39m=\u001b[39m dumps(x, context\u001b[39m=\u001b[39;49mcontext) \u001b[39mif\u001b[39;00m wants_context \u001b[39melse\u001b[39;00m dumps(x)\n\u001b[1;32m    351\u001b[0m     header[\u001b[39m\"\u001b[39m\u001b[39mserializer\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m name\n",
      "File \u001b[0;32m~/mambaforge/envs/dask/lib/python3.9/site-packages/distributed/protocol/serialize.py:73\u001b[0m, in \u001b[0;36mpickle_dumps\u001b[0;34m(x, context)\u001b[0m\n\u001b[1;32m     71\u001b[0m     writeable\u001b[39m.\u001b[39mappend(\u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mreadonly)\n\u001b[0;32m---> 73\u001b[0m frames[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mdumps(\n\u001b[1;32m     74\u001b[0m     x,\n\u001b[1;32m     75\u001b[0m     buffer_callback\u001b[39m=\u001b[39;49mbuffer_callback,\n\u001b[1;32m     76\u001b[0m     protocol\u001b[39m=\u001b[39;49mcontext\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mpickle-protocol\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m) \u001b[39mif\u001b[39;49;00m context \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     77\u001b[0m )\n\u001b[1;32m     78\u001b[0m header \u001b[39m=\u001b[39m {\n\u001b[1;32m     79\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mserializer\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpickle\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     80\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwriteable\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtuple\u001b[39m(writeable),\n\u001b[1;32m     81\u001b[0m }\n",
      "File \u001b[0;32m~/mambaforge/envs/dask/lib/python3.9/site-packages/distributed/protocol/pickle.py:81\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(x, buffer_callback, protocol)\u001b[0m\n\u001b[1;32m     80\u001b[0m     buffers\u001b[39m.\u001b[39mclear()\n\u001b[0;32m---> 81\u001b[0m     result \u001b[39m=\u001b[39m cloudpickle\u001b[39m.\u001b[39;49mdumps(x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdump_kwargs)\n\u001b[1;32m     82\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/dask/lib/python3.9/site-packages/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     70\u001b[0m cp \u001b[39m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m     file, protocol\u001b[39m=\u001b[39mprotocol, buffer_callback\u001b[39m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m cp\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m     74\u001b[0m \u001b[39mreturn\u001b[39;00m file\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/mambaforge/envs/dask/lib/python3.9/site-packages/cloudpickle/cloudpickle_fast.py:632\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39;49mdump(\u001b[39mself\u001b[39;49m, obj)\n\u001b[1;32m    633\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle '_thread.lock' object",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/projects/dask/anndata/anndata/_io/utils.py:246\u001b[0m, in \u001b[0;36mreport_write_key_on_error.<locals>.func_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 246\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    247\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/projects/dask/anndata/anndata/_io/specs/registry.py:311\u001b[0m, in \u001b[0;36mWriter.write_elem\u001b[0;34m(self, store, k, elem, dataset_kwargs, modifiers)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     \u001b[39mreturn\u001b[39;00m write_func(store, k, elem, dataset_kwargs\u001b[39m=\u001b[39;49mdataset_kwargs)\n",
      "File \u001b[0;32m~/projects/dask/anndata/anndata/_io/specs/registry.py:52\u001b[0m, in \u001b[0;36mwrite_spec.<locals>.decorator.<locals>.wrapper\u001b[0;34m(g, k, *args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(g, k, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 52\u001b[0m     result \u001b[39m=\u001b[39m func(g, k, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     53\u001b[0m     g[k]\u001b[39m.\u001b[39mattrs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mencoding-type\u001b[39m\u001b[39m\"\u001b[39m, spec\u001b[39m.\u001b[39mencoding_type)\n",
      "File \u001b[0;32m~/projects/dask/anndata/anndata/_io/specs/methods.py:317\u001b[0m, in \u001b[0;36mwrite_basic_dask\u001b[0;34m(f, k, elem, _writer, dataset_kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m g \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mrequire_dataset(k, shape\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdtype, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs)\n\u001b[0;32m--> 317\u001b[0m da\u001b[39m.\u001b[39;49mstore(elem, g)\n",
      "File \u001b[0;32m~/mambaforge/envs/dask/lib/python3.9/site-packages/dask/array/core.py:1237\u001b[0m, in \u001b[0;36mstore\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1236\u001b[0m store_dsk \u001b[39m=\u001b[39m HighLevelGraph(layers, dependencies)\n\u001b[0;32m-> 1237\u001b[0m compute_as_if_collection(Array, store_dsk, map_keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1238\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dask/lib/python3.9/site-packages/dask/base.py:341\u001b[0m, in \u001b[0;36mcompute_as_if_collection\u001b[0;34m(cls, dsk, keys, scheduler, get, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m dsk2 \u001b[39m=\u001b[39m optimization_function(\u001b[39mcls\u001b[39m)(dsk, keys, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 341\u001b[0m \u001b[39mreturn\u001b[39;00m schedule(dsk2, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/dask/lib/python3.9/site-packages/distributed/client.py:3204\u001b[0m, in \u001b[0;36mClient.get\u001b[0;34m(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   3147\u001b[0m \u001b[39m\"\"\"Compute dask graph\u001b[39;00m\n\u001b[1;32m   3148\u001b[0m \n\u001b[1;32m   3149\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3202\u001b[0m \u001b[39mClient.compute : Compute asynchronous collections\u001b[39;00m\n\u001b[1;32m   3203\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3204\u001b[0m futures \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph_to_futures(\n\u001b[1;32m   3205\u001b[0m     dsk,\n\u001b[1;32m   3206\u001b[0m     keys\u001b[39m=\u001b[39;49m\u001b[39mset\u001b[39;49m(flatten([keys])),\n\u001b[1;32m   3207\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   3208\u001b[0m     allow_other_workers\u001b[39m=\u001b[39;49mallow_other_workers,\n\u001b[1;32m   3209\u001b[0m     resources\u001b[39m=\u001b[39;49mresources,\n\u001b[1;32m   3210\u001b[0m     fifo_timeout\u001b[39m=\u001b[39;49mfifo_timeout,\n\u001b[1;32m   3211\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m   3212\u001b[0m     user_priority\u001b[39m=\u001b[39;49mpriority,\n\u001b[1;32m   3213\u001b[0m     actors\u001b[39m=\u001b[39;49mactors,\n\u001b[1;32m   3214\u001b[0m )\n\u001b[1;32m   3215\u001b[0m packed \u001b[39m=\u001b[39m pack_data(keys, futures)\n",
      "File \u001b[0;32m~/mambaforge/envs/dask/lib/python3.9/site-packages/distributed/client.py:3103\u001b[0m, in \u001b[0;36mClient._graph_to_futures\u001b[0;34m(self, dsk, keys, workers, allow_other_workers, internal_priority, user_priority, resources, retries, fifo_timeout, actors)\u001b[0m\n\u001b[1;32m   3101\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdistributed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotocol\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserialize\u001b[39;00m \u001b[39mimport\u001b[39;00m ToPickle\n\u001b[0;32m-> 3103\u001b[0m header, frames \u001b[39m=\u001b[39m serialize(ToPickle(dsk), on_error\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mraise\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   3104\u001b[0m nbytes \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(header) \u001b[39m+\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlen\u001b[39m, frames))\n",
      "File \u001b[0;32m~/mambaforge/envs/dask/lib/python3.9/site-packages/distributed/protocol/serialize.py:372\u001b[0m, in \u001b[0;36mserialize\u001b[0;34m(x, serializers, on_error, context, iterate_collection)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[39melif\u001b[39;00m on_error \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 372\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg, \u001b[39mstr\u001b[39m(x)[:\u001b[39m10000\u001b[39m]) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# pragma: nocover\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: ('Could not serialize object of type HighLevelGraph', '<ToPickle: HighLevelGraph with 1 layers.\\n<dask.highlevelgraph.HighLevelGraph object at 0x7f59aa12c6d0>\\n 0. 140023082158464\\n>')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m concat_on_disk_wrapper(filepaths, writepth, axis\u001b[39m=\u001b[39;49maxis)\n",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m, in \u001b[0;36mconcat_on_disk_wrapper\u001b[0;34m(filepaths, writepth, axis)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mwith\u001b[39;00m dd\u001b[39m.\u001b[39mClient(cluster) \u001b[39mas\u001b[39;00m client:\n\u001b[1;32m      6\u001b[0m     lock \u001b[39m=\u001b[39m Lock()\n\u001b[0;32m----> 7\u001b[0m     concat_on_disk(filepaths, writepth, axis\u001b[39m=\u001b[39;49maxis, overwrite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/projects/dask/anndata/anndata/experimental/merge.py:591\u001b[0m, in \u001b[0;36mconcat_on_disk\u001b[0;34m(in_files, out_file, overwrite, max_loaded_elems, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise)\u001b[0m\n\u001b[1;32m    587\u001b[0m _write_alt_mapping(groups, output_group, alt_dim, alt_indices, merge)\n\u001b[1;32m    589\u001b[0m \u001b[39m# Write X\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m _write_concat_arrays(\n\u001b[1;32m    592\u001b[0m     arrays\u001b[39m=\u001b[39;49mXs,\n\u001b[1;32m    593\u001b[0m     output_group\u001b[39m=\u001b[39;49moutput_group,\n\u001b[1;32m    594\u001b[0m     output_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    595\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    596\u001b[0m     reindexers\u001b[39m=\u001b[39;49mreindexers,\n\u001b[1;32m    597\u001b[0m     fill_value\u001b[39m=\u001b[39;49mfill_value,\n\u001b[1;32m    598\u001b[0m     max_loaded_elems\u001b[39m=\u001b[39;49mmax_loaded_elems,\n\u001b[1;32m    599\u001b[0m )\n\u001b[1;32m    601\u001b[0m \u001b[39m# Write Layers and {dim}m\u001b[39;00m\n\u001b[1;32m    602\u001b[0m mapping_names \u001b[39m=\u001b[39m [\n\u001b[1;32m    603\u001b[0m     (\n\u001b[1;32m    604\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdim\u001b[39m}\u001b[39;00m\u001b[39mm\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mlayers\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m, axis, reindexers),\n\u001b[1;32m    610\u001b[0m ]\n",
      "File \u001b[0;32m~/projects/dask/anndata/anndata/experimental/merge.py:311\u001b[0m, in \u001b[0;36m_write_concat_arrays\u001b[0;34m(arrays, output_group, output_path, max_loaded_elems, axis, reindexers, fill_value, join)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    308\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConcat of following not supported: \u001b[39m\u001b[39m{\u001b[39;00m[a\u001b[39m.\u001b[39mformat_str \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrays]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m         )\n\u001b[1;32m    310\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     write_concat_dense(\n\u001b[1;32m    312\u001b[0m         arrays, output_group, output_path, axis, reindexers, fill_value\n\u001b[1;32m    313\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/dask/anndata/anndata/experimental/merge.py:191\u001b[0m, in \u001b[0;36mwrite_concat_dense\u001b[0;34m(arrays, output_group, output_path, axis, reindexers, fill_value)\u001b[0m\n\u001b[1;32m    182\u001b[0m darrays \u001b[39m=\u001b[39m (da\u001b[39m.\u001b[39mfrom_array(a, chunks\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrays)\n\u001b[1;32m    184\u001b[0m res \u001b[39m=\u001b[39m da\u001b[39m.\u001b[39mconcatenate(\n\u001b[1;32m    185\u001b[0m     [\n\u001b[1;32m    186\u001b[0m         ri(a, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m axis, fill_value\u001b[39m=\u001b[39mfill_value)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m     axis\u001b[39m=\u001b[39maxis,\n\u001b[1;32m    190\u001b[0m )\n\u001b[0;32m--> 191\u001b[0m write_elem(output_group, output_path, res)\n\u001b[1;32m    192\u001b[0m output_group[output_path]\u001b[39m.\u001b[39mattrs\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m    193\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mencoding-type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mencoding-version\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m0.2.0\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m    194\u001b[0m )\n",
      "File \u001b[0;32m~/projects/dask/anndata/anndata/_io/specs/registry.py:353\u001b[0m, in \u001b[0;36mwrite_elem\u001b[0;34m(store, k, elem, dataset_kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite_elem\u001b[39m(\n\u001b[1;32m    330\u001b[0m     store: GroupStorageType,\n\u001b[1;32m    331\u001b[0m     k: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    334\u001b[0m     dataset_kwargs: Mapping \u001b[39m=\u001b[39m MappingProxyType({}),\n\u001b[1;32m    335\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[39m    Write an element to a storage group using anndata encoding.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[39m        E.g. for zarr this would be `chunks`, `compressor`.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     Writer(_REGISTRY)\u001b[39m.\u001b[39;49mwrite_elem(store, k, elem, dataset_kwargs\u001b[39m=\u001b[39;49mdataset_kwargs)\n",
      "File \u001b[0;32m~/projects/dask/anndata/anndata/_io/utils.py:248\u001b[0m, in \u001b[0;36mreport_write_key_on_error.<locals>.func_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    247\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 248\u001b[0m     re_raise_error(e, elem, key)\n",
      "File \u001b[0;32m~/projects/dask/anndata/anndata/_io/utils.py:229\u001b[0m, in \u001b[0;36mreport_write_key_on_error.<locals>.re_raise_error\u001b[0;34m(e, elem, key)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     parent \u001b[39m=\u001b[39m _get_parent(elem)\n\u001b[0;32m--> 229\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mtype\u001b[39m(e)(\n\u001b[1;32m    230\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAbove error raised while writing key \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m!r}\u001b[39;00m\u001b[39m of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(elem)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto \u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: ('Could not serialize object of type HighLevelGraph', '<ToPickle: HighLevelGraph with 1 layers.\\n<dask.highlevelgraph.HighLevelGraph object at 0x7f59aa12c6d0>\\n 0. 140023082158464\\n>')\n\nAbove error raised while writing key 'X' of <class 'zarr.hierarchy.Group'> to <zarr.storage.DirectoryStore object at 0x7f59aa13b610>"
     ]
    }
   ],
   "source": [
    "concat_on_disk_wrapper(filepaths, writepth, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1c6ec3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tmpdata/02_fat_csr.zarr', 'tmpdata/08_square_csr.zarr'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4defaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
