{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1446b800-7ded-4a4c-b54c-8510100c3bda",
   "metadata": {},
   "source": [
    "# On-Disk Concatenation of AnnData Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4bd620",
   "metadata": {},
   "source": [
    "**Author:** Selman Ã–zleyen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b747f-4384-4b16-8f4f-806edfdc0b06",
   "metadata": {},
   "source": [
    "## Initalizing\n",
    "\n",
    "First let's do our imports and initalize adata objects with the help of the `adata_with_dask` function defined below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7197c",
   "metadata": {},
   "source": [
    "This notebook uses the [memory-profiler](https://pypi.org/project/memory-profiler/) extension, call `pip install memory-profiler` before running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f65fb557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_profiler import memory_usage\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from anndata.tests.helpers import gen_typed_df\n",
    "from anndata.experimental import write_elem\n",
    "import zarr\n",
    "import anndata\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import dask.distributed as dd\n",
    "\n",
    "import anndata\n",
    "import dask.array as da\n",
    "import zarr\n",
    "import gc\n",
    "from anndata.experimental import concat_on_disk\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "\n",
    "OUTDIR = Path(\"tmpdata\")\n",
    "\n",
    "\n",
    "shapes = [\"fat\", \"tall\", \"square\"]\n",
    "sizes = [10_000]\n",
    "densities = [0.1, 1]\n",
    "num_runs = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dd5068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_adata(shape, X):\n",
    "    M, N = shape\n",
    "    obs_names = pd.Index(f\"cell{i}\" for i in range(shape[0]))\n",
    "    var_names = pd.Index(f\"gene{i}\" for i in range(shape[1]))\n",
    "    obs = gen_typed_df(M, obs_names)\n",
    "    var = gen_typed_df(N, var_names)\n",
    "    # For #147\n",
    "    \n",
    "    obs.rename(columns=dict(cat=\"obs_cat\"), inplace=True)\n",
    "    var.rename(columns=dict(cat=\"var_cat\"), inplace=True)\n",
    "    adata = anndata.AnnData(X, obs=obs, var=var)\n",
    "    adata.var_names_make_unique()\n",
    "    adata.obs_names_make_unique()\n",
    "    \n",
    "    return adata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0d9324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_array_funcs_and_names(density):\n",
    "    array_funcs = []\n",
    "    array_names = []\n",
    "    is_dense = density == 1\n",
    "    if is_dense:\n",
    "        array_names.append(\"np\")\n",
    "        array_funcs.append(lambda x: x.toarray())\n",
    "    else:\n",
    "        array_names.extend([\"csc\", \"csr\"])\n",
    "        array_funcs.extend([sparse.csc_matrix, sparse.csr_matrix])\n",
    "    return array_funcs, array_names\n",
    "\n",
    "def generate_dimensions(shape, size):\n",
    "    M = size\n",
    "    N = size\n",
    "    if shape != \"square\":\n",
    "        other_size = size + int(size * np.random.uniform(0.2, 0.4))\n",
    "        if shape == \"fat\":\n",
    "            M = other_size\n",
    "        elif shape == \"tall\":\n",
    "            N = other_size\n",
    "    return M, N\n",
    "\n",
    "def write_data_to_zarr(X, shape, array_name, outdir, file_id):\n",
    "    fname = str(outdir) + f\"/{file_id:02d}_{shape}_{array_name}\"\n",
    "    adata = create_adata((X.shape[0], X.shape[1]), X)\n",
    "    output_zarr_path = f\"{str(fname)}.zarr\"\n",
    "    z = zarr.open_group(output_zarr_path)\n",
    "    write_elem(z, \"/\", adata)\n",
    "    zarr.consolidate_metadata(z.store)\n",
    "    return f\"wrote {X.shape[0]}x{X.shape[1]}_{array_name} -> {fname}\\n\"\n",
    "\n",
    "def write_temp_data(shapes, sizes, densities, num_runs, outdir, rewrite=False):\n",
    "    outdir.mkdir(exist_ok=True)\n",
    "    if rewrite:\n",
    "        (outdir / \"done.txt\").unlink(missing_ok=True)\n",
    "    if (outdir / \"done.txt\").exists():\n",
    "        print(\"already done\")\n",
    "        with open(outdir / \"done.txt\", \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                print(line)\n",
    "        return\n",
    "\n",
    "    saved = []\n",
    "    file_id = 1\n",
    "    for _ in range(num_runs):\n",
    "        for shape in shapes:\n",
    "            for size in sizes:\n",
    "                for density in densities:\n",
    "                    array_funcs, array_names = generate_array_funcs_and_names(density)\n",
    "                    M, N = generate_dimensions(shape, size)\n",
    "\n",
    "                    X_base = sparse.random(M, N, density=density, format=\"csc\")\n",
    "\n",
    "                    for array_func, array_name in zip(array_funcs, array_names):\n",
    "                        X = array_func(X_base)\n",
    "                        report = write_data_to_zarr(X, shape, array_name, outdir, file_id)\n",
    "                        print(report)\n",
    "                        saved.append(report)\n",
    "                        file_id += 1\n",
    "    with open(outdir / \"done.txt\", \"w\") as f:\n",
    "        f.writelines(saved)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbf50dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 12941x10000_csc -> tmpdata/01_fat_csc\n",
      "\n",
      "wrote 12941x10000_csr -> tmpdata/02_fat_csr\n",
      "\n",
      "wrote 12289x10000_np -> tmpdata/03_fat_np\n",
      "\n",
      "wrote 10000x12366_csc -> tmpdata/04_tall_csc\n",
      "\n",
      "wrote 10000x12366_csr -> tmpdata/05_tall_csr\n",
      "\n",
      "wrote 10000x13624_np -> tmpdata/06_tall_np\n",
      "\n",
      "wrote 10000x10000_csc -> tmpdata/07_square_csc\n",
      "\n",
      "wrote 10000x10000_csr -> tmpdata/08_square_csr\n",
      "\n",
      "wrote 10000x10000_np -> tmpdata/09_square_np\n",
      "\n",
      "wrote 13924x10000_csc -> tmpdata/10_fat_csc\n",
      "\n",
      "wrote 13924x10000_csr -> tmpdata/11_fat_csr\n",
      "\n",
      "wrote 13321x10000_np -> tmpdata/12_fat_np\n",
      "\n",
      "wrote 10000x12377_csc -> tmpdata/13_tall_csc\n",
      "\n",
      "wrote 10000x12377_csr -> tmpdata/14_tall_csr\n",
      "\n",
      "wrote 10000x12595_np -> tmpdata/15_tall_np\n",
      "\n",
      "wrote 10000x10000_csc -> tmpdata/16_square_csc\n",
      "\n",
      "wrote 10000x10000_csr -> tmpdata/17_square_csr\n",
      "\n",
      "wrote 10000x10000_np -> tmpdata/18_square_np\n",
      "\n",
      "wrote 13778x10000_csc -> tmpdata/19_fat_csc\n",
      "\n",
      "wrote 13778x10000_csr -> tmpdata/20_fat_csr\n",
      "\n",
      "wrote 12484x10000_np -> tmpdata/21_fat_np\n",
      "\n",
      "wrote 10000x13293_csc -> tmpdata/22_tall_csc\n",
      "\n",
      "wrote 10000x13293_csr -> tmpdata/23_tall_csr\n",
      "\n",
      "wrote 10000x13809_np -> tmpdata/24_tall_np\n",
      "\n",
      "wrote 10000x10000_csc -> tmpdata/25_square_csc\n",
      "\n",
      "wrote 10000x10000_csr -> tmpdata/26_square_csr\n",
      "\n",
      "wrote 10000x10000_np -> tmpdata/27_square_np\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# You can call the function like this:\n",
    "write_temp_data(shapes, sizes, densities, num_runs, OUTDIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f71e387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files by properties\n",
    "filesets = {\n",
    "    'nps' : set(glob.glob(str(OUTDIR) + \"/*np*\")),\n",
    "    'csrs' : set(glob.glob(str(OUTDIR) + \"/*csr*\")),\n",
    "    'cscs' : set(glob.glob(str(OUTDIR) + \"/*csc*\")),\n",
    "    'fats' : set(glob.glob(str(OUTDIR) + \"/*fat*\")),\n",
    "    'talls' : set(glob.glob(str(OUTDIR) + \"/*tall*\")),\n",
    "    'squares' :set(glob.glob(str(OUTDIR) + \"/*square*\")),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfbc8bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_datasets(filesets, requires_reindexing=False):\n",
    "    data = dict()\n",
    "    for fileset, axis in ((\"cscs\",1), (\"csrs\",0), (\"nps\",0), (\"nps\",1)):\n",
    "        filepaths = filesets[fileset].copy()\n",
    "        if not requires_reindexing:\n",
    "            tall_or_fat = filesets['talls'] if axis == 1 else filesets['fats']\n",
    "            filepaths = filepaths.intersection(tall_or_fat.union(filesets['squares']))\n",
    "        fileset_name = \"dense\" if fileset == \"nps\" else \"sparse\"\n",
    "        data[fileset_name, axis] = filepaths\n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7c4769d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('sparse', 1): {'tmpdata/01_fat_csc.zarr',\n",
       "  'tmpdata/04_tall_csc.zarr',\n",
       "  'tmpdata/07_square_csc.zarr',\n",
       "  'tmpdata/10_fat_csc.zarr',\n",
       "  'tmpdata/13_tall_csc.zarr',\n",
       "  'tmpdata/16_square_csc.zarr',\n",
       "  'tmpdata/19_fat_csc.zarr',\n",
       "  'tmpdata/22_tall_csc.zarr',\n",
       "  'tmpdata/25_square_csc.zarr'},\n",
       " ('sparse', 0): {'tmpdata/02_fat_csr.zarr',\n",
       "  'tmpdata/05_tall_csr.zarr',\n",
       "  'tmpdata/08_square_csr.zarr',\n",
       "  'tmpdata/11_fat_csr.zarr',\n",
       "  'tmpdata/14_tall_csr.zarr',\n",
       "  'tmpdata/17_square_csr.zarr',\n",
       "  'tmpdata/20_fat_csr.zarr',\n",
       "  'tmpdata/23_tall_csr.zarr',\n",
       "  'tmpdata/26_square_csr.zarr'},\n",
       " ('dense', 0): {'tmpdata/03_fat_np.zarr',\n",
       "  'tmpdata/06_tall_np.zarr',\n",
       "  'tmpdata/09_square_np.zarr',\n",
       "  'tmpdata/12_fat_np.zarr',\n",
       "  'tmpdata/15_tall_np.zarr',\n",
       "  'tmpdata/18_square_np.zarr',\n",
       "  'tmpdata/21_fat_np.zarr',\n",
       "  'tmpdata/24_tall_np.zarr',\n",
       "  'tmpdata/27_square_np.zarr'},\n",
       " ('dense', 1): {'tmpdata/03_fat_np.zarr',\n",
       "  'tmpdata/06_tall_np.zarr',\n",
       "  'tmpdata/09_square_np.zarr',\n",
       "  'tmpdata/12_fat_np.zarr',\n",
       "  'tmpdata/15_tall_np.zarr',\n",
       "  'tmpdata/18_square_np.zarr',\n",
       "  'tmpdata/21_fat_np.zarr',\n",
       "  'tmpdata/24_tall_np.zarr',\n",
       "  'tmpdata/27_square_np.zarr'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = create_datasets(filesets, requires_reindexing=True)\n",
    "datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aabe8d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense_mem_usage(\n",
    "    filepaths=None,\n",
    "    writepth=None,\n",
    "    axis=None,\n",
    "    max_arg=\"600MiB\",\n",
    "):\n",
    "\n",
    "    cluster = LocalCluster(n_workers=1, threads_per_worker=1, memory_limit=max_arg)\n",
    "    client = Client(cluster)\n",
    "\n",
    "    # get the current memory usage\n",
    "    initial_mem = memory_usage(-1, interval=0.001)[0]\n",
    "\n",
    "    mem_usages = memory_usage(\n",
    "        (\n",
    "            concat_on_disk,\n",
    "            (),\n",
    "            {\n",
    "                \"in_files\": filepaths,\n",
    "                \"out_file\": writepth,\n",
    "                \"axis\": axis,\n",
    "                \"index_unique\": \"-\",\n",
    "            },\n",
    "        ),\n",
    "        include_children=True,\n",
    "        interval=0.001,\n",
    "    )\n",
    "    max_mem = max(mem_usages)\n",
    "    mem_increment = max_mem - initial_mem\n",
    "    \n",
    "    client.close()\n",
    "    cluster.close()\n",
    "    return mem_increment, max_mem, mem_usages, initial_mem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cf0be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_arr_sizes(array_type, filepaths):\n",
    "    res = []\n",
    "    for f in filepaths:\n",
    "        store = zarr.open_group(f).store\n",
    "        additional_size = 0\n",
    "        if array_type == 'sparse':\n",
    "            additional_size = store.getsize('X/data')+store.getsize('X/indices')+store.getsize('X/indptr')\n",
    "        res.append(store.getsize('X')+additional_size)\n",
    "    return res\n",
    "\n",
    "def get_sparse_mem_usage(filepaths, writepth, axis, max_arg):\n",
    "    # get the current memory usage\n",
    "    initial_mem = memory_usage(-1, interval=0.001)[0]\n",
    "\n",
    "    mem_usages = memory_usage(\n",
    "        (\n",
    "            concat_on_disk,\n",
    "            (),\n",
    "            {\n",
    "                \"in_files\": filepaths,\n",
    "                \"out_file\": writepth,\n",
    "                \"axis\": axis,\n",
    "                \"max_loaded_elems\": max_arg,\n",
    "            },\n",
    "        ),\n",
    "        include_children=True,\n",
    "        interval=0.001,\n",
    "    )\n",
    "    max_mem = max(mem_usages)\n",
    "    mem_increment = max_mem - initial_mem\n",
    "    \n",
    "    return mem_increment, max_mem, mem_usages, initial_mem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e0cbd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dataset_max_mem(max_arg, datasets, array_type):\n",
    "    results = {}\n",
    "\n",
    "    for filepaths,axis in [(datasets[array_type,axis],axis) for axis in [0,1]]:\n",
    "        writepth = OUTDIR / f\"{array_type}_{axis}.zarr\"\n",
    "        if writepth.exists():\n",
    "            shutil.rmtree(writepth)\n",
    "\n",
    "        # print the files we are concatenating\n",
    "        print(\"Dataset:\", array_type, axis)\n",
    "        print(f\"Concatenating {len(filepaths)} files with sizes:\")\n",
    "        sizes = get_arr_sizes(array_type, filepaths)\n",
    "        print([str(s//(2**20))+'MiB' for s in sizes])\n",
    "        print(f\"Total size: {sum(sizes)//(2**20)}MiB\")\n",
    "        \n",
    "        mem_usage_func = get_sparse_mem_usage if array_type == 'sparse' else get_dense_mem_usage\n",
    "\n",
    "        # force garbage collection\n",
    "        gc.collect()\n",
    "        # perform profiling\n",
    "        mem_increment, max_mem, mem_usages, initial_mem = mem_usage_func(filepaths, writepth, axis, max_arg)\n",
    "        # force garbage collection again\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"Concatenation finished\")\n",
    "        print(\"Max memory increase:\", int(mem_increment), \"MiB\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "        results[array_type, axis] = {\"max_mem\": max_mem, \"increment\": mem_increment}\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad448529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: sparse 0\n",
      "Concatenating 9 files with sizes:\n",
      "['104MiB', '97MiB', '101MiB', '78MiB', '97MiB', '78MiB', '78MiB', '108MiB', '109MiB']\n",
      "Total size: 853MiB\n",
      "Concatenation finished\n",
      "Max memory increase: 426 MiB\n",
      "--------------------------------------------------\n",
      "Dataset: sparse 1\n",
      "Concatenating 9 files with sizes:\n",
      "['78MiB', '108MiB', '104MiB', '101MiB', '78MiB', '97MiB', '78MiB', '97MiB', '109MiB']\n",
      "Total size: 853MiB\n",
      "Concatenation finished\n",
      "Max memory increase: 590 MiB\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('sparse', 0): {'max_mem': 577.796875, 'increment': 426.50390625},\n",
       " ('sparse', 1): {'max_mem': 756.609375, 'increment': 590.16015625}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_max_mem(max_arg=1_000_000_000, datasets=datasets, array_type='sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aa2c05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: sparse 0\n",
      "Concatenating 9 files with sizes:\n",
      "['104MiB', '97MiB', '101MiB', '78MiB', '97MiB', '78MiB', '78MiB', '108MiB', '109MiB']\n",
      "Total size: 853MiB\n",
      "Concatenation finished\n",
      "Max memory increase: 427 MiB\n",
      "--------------------------------------------------\n",
      "Dataset: sparse 1\n",
      "Concatenating 9 files with sizes:\n",
      "['78MiB', '108MiB', '104MiB', '101MiB', '78MiB', '97MiB', '78MiB', '97MiB', '109MiB']\n",
      "Total size: 853MiB\n",
      "Concatenation finished\n",
      "Max memory increase: 589 MiB\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('sparse', 0): {'max_mem': 596.15625, 'increment': 427.5},\n",
       " ('sparse', 1): {'max_mem': 759.921875, 'increment': 589.625}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_max_mem(max_arg=100_000_000, datasets=datasets, array_type='sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3b74ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: dense 0\n",
      "Concatenating 9 files with sizes:\n",
      "['910MiB', '668MiB', '923MiB', '891MiB', '668MiB', '822MiB', '835MiB', '843MiB', '668MiB']\n",
      "Total size: 7233MiB\n",
      "Concatenation finished\n",
      "Max memory increase: 1023 MiB\n",
      "--------------------------------------------------\n",
      "Dataset: dense 1\n",
      "Concatenating 9 files with sizes:\n",
      "['910MiB', '668MiB', '923MiB', '891MiB', '668MiB', '822MiB', '835MiB', '843MiB', '668MiB']\n",
      "Total size: 7233MiB\n",
      "Concatenation finished\n",
      "Max memory increase: 967 MiB\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('dense', 0): {'max_mem': 1210.96875, 'increment': 1023.703125},\n",
       " ('dense', 1): {'max_mem': 1163.109375, 'increment': 967.73046875}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_max_mem(max_arg=\"1000MiB\", datasets=datasets, array_type='dense')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
